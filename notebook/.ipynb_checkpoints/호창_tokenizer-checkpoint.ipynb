{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28be367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "## 경고 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## 시각화 툴\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## encoding \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## vocabulary\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "## preprocessor\n",
    "from custom_preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3637126",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "preprocessed_train, preprocessed_test = preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f75524ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train[\"conversation\"].apply(lambda x: len(x.split())).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74accc02",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40d809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS_NAMES에 '일반 대화'를 포함시킴\n",
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '일반 대화']\n",
    "\n",
    "# 수동 매핑 설정\n",
    "class_mapping = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}\n",
    "\n",
    "# 'class' 열을 수동 매핑 적용하기 전에 문자열로 변환\n",
    "preprocessed_train['class'] = preprocessed_train['class'].astype(str).map(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c958e4",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "260302b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b194a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# 특수 토큰 ID 확인\n",
    "CLS_TOKEN_ID = tokenizer.cls_token_id  # [CLS] 토큰 ID\n",
    "SEP_TOKEN_ID = tokenizer.sep_token_id  # [SEP] 토큰 ID\n",
    "UNK_TOKEN_ID = tokenizer.unk_token_id  # [UNK] 토큰 ID\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id  # [PAD] 토큰 ID\n",
    "\n",
    "# 각 문장에 [CLS], [SEP] 추가\n",
    "def add_special_tokens(tokenized_texts):\n",
    "    return [[CLS_TOKEN_ID] + tokenizer.convert_tokens_to_ids(tokens) + [SEP_TOKEN_ID] for tokens in tokenized_texts]\n",
    "\n",
    "## 토크나이징\n",
    "tokenized_train = [tokenizer.tokenize(con) for con in preprocessed_train['conversation'].tolist()]\n",
    "tokenized_test = [tokenizer.tokenize(con) for con in preprocessed_test['text'].tolist()]\n",
    "\n",
    "## [CLS], [SEP] 토큰 추가\n",
    "token_id_train = add_special_tokens(tokenized_train)\n",
    "token_id_test = add_special_tokens(tokenized_test)\n",
    "\n",
    "# 패딩과 트렁케이션 설정\n",
    "## 문장의 max length를 test 문장의 최대 길이로 설정한다. \n",
    "## padding은 앞에서부터 진행\n",
    "## max길이를 넘어가면 앞에서부터 자른다. -> 한국말은 끝까지 들어야하므로..\n",
    "token_id_train = pad_sequences(token_id_train, maxlen=191, dtype='long', padding='pre', truncating='pre', value=PAD_TOKEN_ID)\n",
    "token_id_test = pad_sequences(token_id_test, maxlen=191, dtype='long', padding='pre', truncating='pre', value=PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3de254bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33342a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
