{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d65d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../model\")\n",
    "\n",
    "## 경고 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## 시각화 툴\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## encoding \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## vocabulary\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "## tokenizer \n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## preprocessor\n",
    "from custom_preprocessor import Preprocessor\n",
    "\n",
    "## model\n",
    "from transformer_v1 import transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf35f36",
   "metadata": {},
   "source": [
    "# hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7726c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 파라미터\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# 학습 파라미터\n",
    "EPOCHS = 10\n",
    "\n",
    "# 모델 구조 파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3ee7e",
   "metadata": {},
   "source": [
    "# Data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff21e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "preprocessed_train, preprocessed_test = preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ada7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train[\"conversation\"].apply(lambda x: len(x.split())).max()\n",
    "\n",
    "## CLASS_NAMES에 '일반 대화'를 포함시킴\n",
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '일반 대화']\n",
    "\n",
    "# 수동 매핑 설정\n",
    "class_mapping = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}\n",
    "\n",
    "# 'class' 열을 수동 매핑 적용하기 전에 문자열로 변환\n",
    "preprocessed_train['class'] = preprocessed_train['class'].astype(str).map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0095aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# 특수 토큰 ID 확인\n",
    "CLS_TOKEN_ID = tokenizer.cls_token_id  # [CLS] 토큰 ID\n",
    "SEP_TOKEN_ID = tokenizer.sep_token_id  # [SEP] 토큰 ID\n",
    "UNK_TOKEN_ID = tokenizer.unk_token_id  # [UNK] 토큰 ID\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id  # [PAD] 토큰 ID\n",
    "\n",
    "# 각 문장에 [CLS], [SEP] 추가\n",
    "def add_special_tokens(tokenized_texts):\n",
    "    return [[CLS_TOKEN_ID] + tokenizer.convert_tokens_to_ids(tokens) + [SEP_TOKEN_ID] for tokens in tokenized_texts]\n",
    "\n",
    "## 토크나이징\n",
    "tokenized_train = [tokenizer.tokenize(con) for con in preprocessed_train['conversation'].tolist()]\n",
    "tokenized_test = [tokenizer.tokenize(con) for con in preprocessed_test['text'].tolist()]\n",
    "\n",
    "## [CLS], [SEP] 토큰 추가\n",
    "token_id_train = add_special_tokens(tokenized_train)\n",
    "token_id_test = add_special_tokens(tokenized_test)\n",
    "\n",
    "# 패딩과 트렁케이션 설정\n",
    "## 문장의 max length를 test 문장의 최대 길이로 설정한다. \n",
    "## padding은 앞에서부터 진행\n",
    "## max길이를 넘어가면 앞에서부터 자른다. -> 한국말은 끝까지 들어야하므로..\n",
    "token_id_train = pad_sequences(token_id_train, maxlen=191, dtype='long', padding='pre', truncating='pre', value=PAD_TOKEN_ID)\n",
    "token_id_test = pad_sequences(token_id_test, maxlen=191, dtype='long', padding='pre', truncating='pre', value=PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91dcd11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4976, 191)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c5830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = token_id_train[:3500]\n",
    "y_train = preprocessed_train['class'][:3500]\n",
    "X_val = token_id_train[3500:3850]\n",
    "y_val = preprocessed_train['class'][3500:3850]\n",
    "X_test = token_id_train[3850:]\n",
    "y_test = preprocessed_train['class'][3850:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d3734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 및 버퍼 크기\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# 학습용 데이터셋을 tf.data.Dataset으로 변환\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': X_train,   # 대화 데이터\n",
    "    },\n",
    "    {\n",
    "        'outputs': y_train,  # 분류 레이블\n",
    "    }\n",
    "))\n",
    "\n",
    "# 검증용 데이터셋을 tf.data.Dataset으로 변환\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': X_val,\n",
    "    },\n",
    "    {\n",
    "        'outputs': y_val,\n",
    "    }\n",
    "))\n",
    "\n",
    "# 테스트용 데이터셋을 tf.data.Dataset으로 변환\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': X_test,\n",
    "    },\n",
    "    {\n",
    "        'outputs': y_test,\n",
    "    }\n",
    "))\n",
    "\n",
    "# 데이터셋을 섞고, 배치 처리\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f976bf3",
   "metadata": {},
   "source": [
    "# Model 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "241fd0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    1310208     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 256)          0           encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "linear_hidden_layer (Dense)     (None, 1028)         264196      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 5)            5145        linear_hidden_layer[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,579,549\n",
      "Trainable params: 1,579,549\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "VOCAB_SIZE = 1000\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46278caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.squeeze(y_true, axis=-1)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False)(y_true, y_pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d892a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a48f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6a0b5",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd4bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 8s 76ms/step - loss: 1.4864 - accuracy: 0.2540 - val_loss: 1.4063 - val_accuracy: 0.2571\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 4s 67ms/step - loss: 1.3853 - accuracy: 0.2977 - val_loss: 1.4100 - val_accuracy: 0.2457\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 4s 67ms/step - loss: 1.3792 - accuracy: 0.2863 - val_loss: 1.3778 - val_accuracy: 0.2857\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 4s 67ms/step - loss: 1.3754 - accuracy: 0.2889 - val_loss: 1.3815 - val_accuracy: 0.3114\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 4s 67ms/step - loss: 1.3682 - accuracy: 0.2926 - val_loss: 1.3730 - val_accuracy: 0.3057\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 4s 67ms/step - loss: 1.3685 - accuracy: 0.3057 - val_loss: 1.3515 - val_accuracy: 0.3371\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 4s 68ms/step - loss: 1.3714 - accuracy: 0.2900 - val_loss: 1.3677 - val_accuracy: 0.3000\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 4s 68ms/step - loss: 1.3655 - accuracy: 0.3071 - val_loss: 1.3794 - val_accuracy: 0.2886\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 4s 68ms/step - loss: 1.3649 - accuracy: 0.3103 - val_loss: 1.3528 - val_accuracy: 0.3457\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 4s 68ms/step - loss: 1.3724 - accuracy: 0.3054 - val_loss: 1.3954 - val_accuracy: 0.2714\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = EPOCHS\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,  # 검증 데이터셋 추가\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692f9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 25ms/step - loss: 9.5089 - accuracy: 0.0346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.508933067321777, 0.03463587909936905]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81796e7c",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018d4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_vec = model(token_id_test)\n",
    "y_pred = np.argmax(result_vec, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a08cbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(y_pred, user_name, f1_score=None):\n",
    "    data_path =\"/aiffel/aiffel/dlthon_team5/data\"\n",
    "    save_path =\"/aiffel/aiffel/dlthon_team5/submission\"\n",
    "    submission_path = join(data_path, 'new_submission.csv')\n",
    "    submission = pd.read_csv(submission_path)\n",
    "    submission['class'] = y_pred\n",
    "    submission_csv_path = '{}/submission_{}_f1score_{}.csv'.format(save_path, user_name, f1_score)\n",
    "    submission.to_csv(submission_csv_path, index=False)\n",
    "    print('{} saved!'.format(submission_csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15f8209c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/dlthon_team5/submission/submission_phc_v1_f1score_0.03.csv saved!\n"
     ]
    }
   ],
   "source": [
    "save_submission(y_pred,'phc_v1','0.03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab58e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
