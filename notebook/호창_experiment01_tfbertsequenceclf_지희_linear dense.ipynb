{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9903d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../model\")\n",
    "\n",
    "## 경고 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## 시각화 툴\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## encoding \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "## vocabulary\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "## tokenizer \n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## preprocessor\n",
    "from custom_preprocessor import Preprocessor\n",
    "\n",
    "## model\n",
    "from transformer_v1 import transformer\n",
    "from transformers import TFBertForSequenceClassification, BertConfig\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b685aa",
   "metadata": {},
   "source": [
    "# Data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f8fa411",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "preprocessed_train, preprocessed_test = preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bc05ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train[\"conversation\"].apply(lambda x: len(x.split())).max()\n",
    "\n",
    "## CLASS_NAMES에 '일반 대화'를 포함시킴\n",
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화', '일반 대화']\n",
    "\n",
    "# 수동 매핑 설정\n",
    "class_mapping = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}\n",
    "\n",
    "\n",
    "# 'class' 열을 수동 매핑 적용하기 전에 문자열로 변환\n",
    "preprocessed_train['class'] = preprocessed_train['class'].astype(str).map(class_mapping)\n",
    "labels = preprocessed_train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abea2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_list = []\n",
    "for conv in preprocessed_train['conversation']:\n",
    "    conversations_list.append(conv)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6668ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size_rate = 0.2\n",
    "val_size_rate = 0.1\n",
    "\n",
    "# 시드값 설정\n",
    "seed = 1004\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    conversations_list, \n",
    "    labels, \n",
    "    test_size=test_size_rate, \n",
    "    stratify=labels, \n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    test_size=val_size_rate, \n",
    "    stratify=y_train, \n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f1f19cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582 (3582,)\n",
      "398 (398,)\n",
      "996 (996,)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), y_train.shape)\n",
    "print(len(X_valid), y_valid.shape)\n",
    "print(len(X_test), y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa02f8",
   "metadata": {},
   "source": [
    "## 토큰화 및 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dbe3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "token_X_train = tokenizer(\n",
    "    X_train, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=191, \n",
    ") \n",
    "\n",
    "token_X_valid = tokenizer(\n",
    "    X_valid, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=191, \n",
    ") \n",
    "\n",
    "token_X_test = tokenizer(\n",
    "    X_test, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=191, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e57a7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# 클래스 가중치 계산 (클래스 4 포함)\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# 각 샘플에 가중치 할당\n",
    "sample_weights = np.array([class_weights[y] for y in y_train])\n",
    "\n",
    "# from_tensor_slices에서 입력을 딕셔너리로 전달\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(token_X_train),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(token_X_valid),\n",
    "    y_valid\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(token_X_test),\n",
    "    y_test\n",
    "))\n",
    "\n",
    "# 데이터셋을 섞고, 배치 처리\n",
    "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e498b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4d77d099ec4a2e946dccbf65d81949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/klue/bert-base/resolve/main/tf_model.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load weights for 'klue/bert-base'. Make sure that:\n\n- 'klue/bert-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'klue/bert-base' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[1;32m   1393\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1403\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1573\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1575\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/klue/bert-base/resolve/main/tf_model.h5",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4849/3354700914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the pretrained BERT model for sequence classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'klue/bert-base'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Freeze all layers except the classification layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                     \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {TF2_WEIGHTS_NAME}, {WEIGHTS_NAME}.\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                 )\n\u001b[0;32m-> 1409\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading weights file {archive_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load weights for 'klue/bert-base'. Make sure that:\n\n- 'klue/bert-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'klue/bert-base' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pretrained BERT model for sequence classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('klue/bert-base', num_labels=2)\n",
    "\n",
    "# Freeze all layers except the classification layer\n",
    "for layer in model.layers:\n",
    "    if 'bert' in layer.name:\n",
    "        layer.trainable = False\n",
    "\n",
    "# Ensure dropout is disabled\n",
    "for layer in model.layers:\n",
    "    if 'dropout' in layer.name:\n",
    "        layer.rate = 0.0  # Disable dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#지희 수정\n",
    "model = TFBertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", \n",
    "    num_labels=5\n",
    ")\n",
    "\n",
    "for layer in model.layers:\n",
    "    if 'bert' in layer.name:\n",
    "        layer.trainable = False\n",
    "    elif 'dropout' in layer.name:\n",
    "        layer.trainable = False  \n",
    "        \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a96cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_function(y_true, y_pred):\n",
    "#     y_true = tf.squeeze(y_true, axis=-1)\n",
    "#     loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#         from_logits=False)(y_true, y_pred)\n",
    "#     return tf.reduce_mean(loss)\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5),K.floatx())\n",
    "    true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.clip(y_true, 0, 1))\n",
    "    recall_ratio = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_ratio\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5), K.floatx())\n",
    "    true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(y_pred)\n",
    "    precision_ratio = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_ratio\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    y_pred_classes = tf.argmax(y_pred, axis=-1)\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c26a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "#     def __init__(self, d_model, warmup_steps=4000):\n",
    "#         super(CustomSchedule, self).__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "#         self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "#         self.warmup_steps = warmup_steps\n",
    "\n",
    "#     def __call__(self, step):\n",
    "#         arg1 = tf.math.rsqrt(step)\n",
    "#         arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42460b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 16\n",
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c438e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=lr, \n",
    "#     beta_1=0.9, \n",
    "#     beta_2=0.98, \n",
    "#     epsilon=1e-9\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    metrics=[f1_m]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b3aaf",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9966f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = \"val_loss\", \n",
    "    patience = 2, \n",
    "    mode = \"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fee52a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "224/224 [==============================] - 66s 251ms/step - loss: 1.6120 - f1_m: 0.0000e+00 - val_loss: 1.6067 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/500\n",
      "224/224 [==============================] - 54s 242ms/step - loss: 1.6061 - f1_m: 0.0000e+00 - val_loss: 1.6000 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/500\n",
      "224/224 [==============================] - 55s 245ms/step - loss: 1.5993 - f1_m: 0.0000e+00 - val_loss: 1.5936 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/500\n",
      "224/224 [==============================] - 56s 249ms/step - loss: 1.5881 - f1_m: 0.0000e+00 - val_loss: 1.5821 - val_f1_m: 0.0000e+00\n",
      "Epoch 6/500\n",
      "224/224 [==============================] - 56s 250ms/step - loss: 1.5856 - f1_m: 0.0000e+00 - val_loss: 1.5766 - val_f1_m: 0.0000e+00\n",
      "Epoch 7/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.5804 - f1_m: 0.0000e+00 - val_loss: 1.5712 - val_f1_m: 0.0000e+00\n",
      "Epoch 8/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5743 - f1_m: 0.0000e+00 - val_loss: 1.5659 - val_f1_m: 0.0000e+00\n",
      "Epoch 9/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5726 - f1_m: 0.0000e+00 - val_loss: 1.5607 - val_f1_m: 0.0000e+00\n",
      "Epoch 10/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5647 - f1_m: 0.0023 - val_loss: 1.5554 - val_f1_m: 0.0000e+00\n",
      "Epoch 11/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5611 - f1_m: 0.0061 - val_loss: 1.5503 - val_f1_m: 0.0000e+00\n",
      "Epoch 12/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5563 - f1_m: 0.0091 - val_loss: 1.5452 - val_f1_m: 0.0000e+00\n",
      "Epoch 13/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.5513 - f1_m: 0.0118 - val_loss: 1.5402 - val_f1_m: 0.0104\n",
      "Epoch 14/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.5489 - f1_m: 0.0220 - val_loss: 1.5353 - val_f1_m: 0.0166\n",
      "Epoch 15/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5443 - f1_m: 0.0228 - val_loss: 1.5305 - val_f1_m: 0.0467\n",
      "Epoch 16/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5363 - f1_m: 0.0331 - val_loss: 1.5258 - val_f1_m: 0.0737\n",
      "Epoch 17/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5276 - f1_m: 0.0767 - val_loss: 1.5123 - val_f1_m: 0.1510\n",
      "Epoch 20/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5255 - f1_m: 0.0810 - val_loss: 1.5077 - val_f1_m: 0.1554\n",
      "Epoch 21/500\n",
      "224/224 [==============================] - 57s 252ms/step - loss: 1.5193 - f1_m: 0.0757 - val_loss: 1.5033 - val_f1_m: 0.1629\n",
      "Epoch 22/500\n",
      "224/224 [==============================] - 57s 252ms/step - loss: 1.5162 - f1_m: 0.1027 - val_loss: 1.4990 - val_f1_m: 0.1934\n",
      "Epoch 23/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5080 - f1_m: 0.1157 - val_loss: 1.4908 - val_f1_m: 0.2307\n",
      "Epoch 25/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5079 - f1_m: 0.1302 - val_loss: 1.4867 - val_f1_m: 0.2307\n",
      "Epoch 26/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.5020 - f1_m: 0.1338 - val_loss: 1.4829 - val_f1_m: 0.2397\n",
      "Epoch 27/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4978 - f1_m: 0.1522 - val_loss: 1.4789 - val_f1_m: 0.2517\n",
      "Epoch 28/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4966 - f1_m: 0.1576 - val_loss: 1.4750 - val_f1_m: 0.2517\n",
      "Epoch 29/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4950 - f1_m: 0.1662 - val_loss: 1.4708 - val_f1_m: 0.2578\n",
      "Epoch 30/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4922 - f1_m: 0.1700 - val_loss: 1.4670 - val_f1_m: 0.2612\n",
      "Epoch 31/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4853 - f1_m: 0.1867 - val_loss: 1.4633 - val_f1_m: 0.2661\n",
      "Epoch 32/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4851 - f1_m: 0.1759 - val_loss: 1.4597 - val_f1_m: 0.2841\n",
      "Epoch 33/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4785 - f1_m: 0.1856 - val_loss: 1.4561 - val_f1_m: 0.2841\n",
      "Epoch 34/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4776 - f1_m: 0.2043 - val_loss: 1.4526 - val_f1_m: 0.2841\n",
      "Epoch 35/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4735 - f1_m: 0.2056 - val_loss: 1.4491 - val_f1_m: 0.2873\n",
      "Epoch 36/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4720 - f1_m: 0.2085 - val_loss: 1.4457 - val_f1_m: 0.2873\n",
      "Epoch 37/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4671 - f1_m: 0.2204 - val_loss: 1.4423 - val_f1_m: 0.2993\n",
      "Epoch 38/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4665 - f1_m: 0.2127 - val_loss: 1.4390 - val_f1_m: 0.2993\n",
      "Epoch 39/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4640 - f1_m: 0.2299 - val_loss: 1.4360 - val_f1_m: 0.3025\n",
      "Epoch 40/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4602 - f1_m: 0.2341 - val_loss: 1.4327 - val_f1_m: 0.3092\n",
      "Epoch 41/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4557 - f1_m: 0.2426 - val_loss: 1.4295 - val_f1_m: 0.3092\n",
      "Epoch 42/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4536 - f1_m: 0.2460 - val_loss: 1.4264 - val_f1_m: 0.3186\n",
      "Epoch 43/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4504 - f1_m: 0.2590 - val_loss: 1.4232 - val_f1_m: 0.3234\n",
      "Epoch 44/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4455 - f1_m: 0.2622 - val_loss: 1.4201 - val_f1_m: 0.3229\n",
      "Epoch 45/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4427 - f1_m: 0.2645 - val_loss: 1.4170 - val_f1_m: 0.3277\n",
      "Epoch 46/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4392 - f1_m: 0.2742 - val_loss: 1.4139 - val_f1_m: 0.3277\n",
      "Epoch 47/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4384 - f1_m: 0.2741 - val_loss: 1.4109 - val_f1_m: 0.3312\n",
      "Epoch 48/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4346 - f1_m: 0.2830 - val_loss: 1.4079 - val_f1_m: 0.3312\n",
      "Epoch 49/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4364 - f1_m: 0.2806 - val_loss: 1.4049 - val_f1_m: 0.3312\n",
      "Epoch 50/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4299 - f1_m: 0.2681 - val_loss: 1.4021 - val_f1_m: 0.3312\n",
      "Epoch 51/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4267 - f1_m: 0.2825 - val_loss: 1.3991 - val_f1_m: 0.3312\n",
      "Epoch 52/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4282 - f1_m: 0.2785 - val_loss: 1.3963 - val_f1_m: 0.3312\n",
      "Epoch 53/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4239 - f1_m: 0.2856 - val_loss: 1.3938 - val_f1_m: 0.3375\n",
      "Epoch 54/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4160 - f1_m: 0.2954 - val_loss: 1.3911 - val_f1_m: 0.3417\n",
      "Epoch 55/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4178 - f1_m: 0.2910 - val_loss: 1.3885 - val_f1_m: 0.3417\n",
      "Epoch 56/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4200 - f1_m: 0.2898 - val_loss: 1.3858 - val_f1_m: 0.3417\n",
      "Epoch 57/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4178 - f1_m: 0.2986 - val_loss: 1.3833 - val_f1_m: 0.3417\n",
      "Epoch 58/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4123 - f1_m: 0.3055 - val_loss: 1.3806 - val_f1_m: 0.3417\n",
      "Epoch 59/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4095 - f1_m: 0.3059 - val_loss: 1.3781 - val_f1_m: 0.3417\n",
      "Epoch 60/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4073 - f1_m: 0.3045 - val_loss: 1.3756 - val_f1_m: 0.3417\n",
      "Epoch 61/500\n",
      "224/224 [==============================] - 56s 250ms/step - loss: 1.4092 - f1_m: 0.3201 - val_loss: 1.3730 - val_f1_m: 0.3442\n",
      "Epoch 62/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4034 - f1_m: 0.3084 - val_loss: 1.3705 - val_f1_m: 0.3442\n",
      "Epoch 63/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.4003 - f1_m: 0.3213 - val_loss: 1.3682 - val_f1_m: 0.3442\n",
      "Epoch 64/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.4026 - f1_m: 0.3259 - val_loss: 1.3658 - val_f1_m: 0.3442\n",
      "Epoch 65/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3942 - f1_m: 0.3367 - val_loss: 1.3635 - val_f1_m: 0.3442\n",
      "Epoch 66/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3935 - f1_m: 0.3230 - val_loss: 1.3612 - val_f1_m: 0.3486\n",
      "Epoch 67/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3927 - f1_m: 0.3338 - val_loss: 1.3588 - val_f1_m: 0.3442\n",
      "Epoch 68/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3897 - f1_m: 0.3317 - val_loss: 1.3565 - val_f1_m: 0.3486\n",
      "Epoch 69/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3864 - f1_m: 0.3289 - val_loss: 1.3541 - val_f1_m: 0.3486\n",
      "Epoch 70/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3898 - f1_m: 0.3363 - val_loss: 1.3519 - val_f1_m: 0.3486\n",
      "Epoch 71/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3853 - f1_m: 0.3509 - val_loss: 1.3498 - val_f1_m: 0.3486\n",
      "Epoch 72/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3891 - f1_m: 0.3409 - val_loss: 1.3478 - val_f1_m: 0.3486\n",
      "Epoch 73/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3774 - f1_m: 0.3594 - val_loss: 1.3459 - val_f1_m: 0.3486\n",
      "Epoch 74/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3823 - f1_m: 0.3455 - val_loss: 1.3437 - val_f1_m: 0.3486\n",
      "Epoch 75/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3783 - f1_m: 0.3480 - val_loss: 1.3416 - val_f1_m: 0.3546\n",
      "Epoch 76/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3783 - f1_m: 0.3556 - val_loss: 1.3395 - val_f1_m: 0.3593\n",
      "Epoch 77/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3770 - f1_m: 0.3565 - val_loss: 1.3372 - val_f1_m: 0.3546\n",
      "Epoch 78/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3696 - f1_m: 0.3779 - val_loss: 1.3355 - val_f1_m: 0.3626\n",
      "Epoch 79/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3680 - f1_m: 0.3692 - val_loss: 1.3333 - val_f1_m: 0.3626\n",
      "Epoch 80/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3698 - f1_m: 0.3614 - val_loss: 1.3312 - val_f1_m: 0.3626\n",
      "Epoch 81/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3657 - f1_m: 0.3626 - val_loss: 1.3294 - val_f1_m: 0.3664\n",
      "Epoch 82/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3619 - f1_m: 0.3660 - val_loss: 1.3276 - val_f1_m: 0.3707\n",
      "Epoch 83/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3627 - f1_m: 0.3673 - val_loss: 1.3255 - val_f1_m: 0.3758\n",
      "Epoch 84/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3601 - f1_m: 0.3666 - val_loss: 1.3234 - val_f1_m: 0.3758\n",
      "Epoch 85/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3623 - f1_m: 0.3664 - val_loss: 1.3216 - val_f1_m: 0.3758\n",
      "Epoch 86/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3578 - f1_m: 0.3660 - val_loss: 1.3197 - val_f1_m: 0.3758\n",
      "Epoch 87/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3551 - f1_m: 0.3832 - val_loss: 1.3177 - val_f1_m: 0.3790\n",
      "Epoch 88/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3499 - f1_m: 0.3932 - val_loss: 1.3161 - val_f1_m: 0.3790\n",
      "Epoch 89/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3506 - f1_m: 0.3787 - val_loss: 1.3144 - val_f1_m: 0.3790\n",
      "Epoch 90/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3482 - f1_m: 0.3895 - val_loss: 1.3125 - val_f1_m: 0.3790\n",
      "Epoch 91/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3482 - f1_m: 0.3890 - val_loss: 1.3108 - val_f1_m: 0.3790\n",
      "Epoch 92/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3497 - f1_m: 0.3827 - val_loss: 1.3091 - val_f1_m: 0.3847\n",
      "Epoch 93/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3455 - f1_m: 0.3989 - val_loss: 1.3075 - val_f1_m: 0.3847\n",
      "Epoch 94/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3454 - f1_m: 0.3979 - val_loss: 1.3057 - val_f1_m: 0.3790\n",
      "Epoch 95/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3447 - f1_m: 0.3999 - val_loss: 1.3043 - val_f1_m: 0.3790\n",
      "Epoch 96/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3377 - f1_m: 0.4132 - val_loss: 1.3027 - val_f1_m: 0.3840\n",
      "Epoch 97/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3350 - f1_m: 0.3997 - val_loss: 1.3012 - val_f1_m: 0.3864\n",
      "Epoch 98/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3398 - f1_m: 0.4023 - val_loss: 1.2996 - val_f1_m: 0.3864\n",
      "Epoch 99/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3362 - f1_m: 0.4089 - val_loss: 1.2976 - val_f1_m: 0.3840\n",
      "Epoch 100/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3328 - f1_m: 0.4093 - val_loss: 1.2963 - val_f1_m: 0.3864\n",
      "Epoch 101/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3304 - f1_m: 0.4152 - val_loss: 1.2946 - val_f1_m: 0.3864\n",
      "Epoch 102/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3307 - f1_m: 0.4177 - val_loss: 1.2929 - val_f1_m: 0.3951\n",
      "Epoch 103/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3287 - f1_m: 0.4221 - val_loss: 1.2914 - val_f1_m: 0.3901\n",
      "Epoch 104/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3228 - f1_m: 0.4151 - val_loss: 1.2900 - val_f1_m: 0.3901\n",
      "Epoch 105/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3291 - f1_m: 0.4143 - val_loss: 1.2878 - val_f1_m: 0.3952\n",
      "Epoch 106/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3289 - f1_m: 0.4294 - val_loss: 1.2865 - val_f1_m: 0.3952\n",
      "Epoch 107/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3227 - f1_m: 0.4251 - val_loss: 1.2851 - val_f1_m: 0.3952\n",
      "Epoch 108/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3174 - f1_m: 0.4467 - val_loss: 1.2837 - val_f1_m: 0.3952\n",
      "Epoch 109/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3174 - f1_m: 0.4308 - val_loss: 1.2822 - val_f1_m: 0.3980\n",
      "Epoch 110/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3138 - f1_m: 0.4539 - val_loss: 1.2807 - val_f1_m: 0.4012\n",
      "Epoch 111/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3212 - f1_m: 0.4378 - val_loss: 1.2793 - val_f1_m: 0.4036\n",
      "Epoch 112/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3165 - f1_m: 0.4471 - val_loss: 1.2778 - val_f1_m: 0.4012\n",
      "Epoch 113/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3135 - f1_m: 0.4462 - val_loss: 1.2762 - val_f1_m: 0.4012\n",
      "Epoch 114/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3139 - f1_m: 0.4443 - val_loss: 1.2749 - val_f1_m: 0.4012\n",
      "Epoch 115/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3088 - f1_m: 0.4533 - val_loss: 1.2737 - val_f1_m: 0.4036\n",
      "Epoch 116/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3151 - f1_m: 0.4472 - val_loss: 1.2722 - val_f1_m: 0.4293\n",
      "Epoch 117/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3077 - f1_m: 0.4403 - val_loss: 1.2711 - val_f1_m: 0.4186\n",
      "Epoch 118/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3113 - f1_m: 0.4572 - val_loss: 1.2695 - val_f1_m: 0.4233\n",
      "Epoch 119/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.3102 - f1_m: 0.4728 - val_loss: 1.2681 - val_f1_m: 0.4306\n",
      "Epoch 120/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3037 - f1_m: 0.4632 - val_loss: 1.2667 - val_f1_m: 0.4306\n",
      "Epoch 121/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3068 - f1_m: 0.4569 - val_loss: 1.2653 - val_f1_m: 0.4344\n",
      "Epoch 122/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3007 - f1_m: 0.4558 - val_loss: 1.2641 - val_f1_m: 0.4402\n",
      "Epoch 123/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3091 - f1_m: 0.4606 - val_loss: 1.2627 - val_f1_m: 0.4337\n",
      "Epoch 124/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.3000 - f1_m: 0.4671 - val_loss: 1.2616 - val_f1_m: 0.4423\n",
      "Epoch 125/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2975 - f1_m: 0.4644 - val_loss: 1.2602 - val_f1_m: 0.4494\n",
      "Epoch 126/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2984 - f1_m: 0.4760 - val_loss: 1.2588 - val_f1_m: 0.4470\n",
      "Epoch 127/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2969 - f1_m: 0.4585 - val_loss: 1.2575 - val_f1_m: 0.4470\n",
      "Epoch 128/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2929 - f1_m: 0.4668 - val_loss: 1.2562 - val_f1_m: 0.4470\n",
      "Epoch 129/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2996 - f1_m: 0.4683 - val_loss: 1.2550 - val_f1_m: 0.4470\n",
      "Epoch 130/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2952 - f1_m: 0.4727 - val_loss: 1.2537 - val_f1_m: 0.4463\n",
      "Epoch 131/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2921 - f1_m: 0.4786 - val_loss: 1.2531 - val_f1_m: 0.4421\n",
      "Epoch 132/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2933 - f1_m: 0.4793 - val_loss: 1.2516 - val_f1_m: 0.4421\n",
      "Epoch 133/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2900 - f1_m: 0.4682 - val_loss: 1.2505 - val_f1_m: 0.4421\n",
      "Epoch 134/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2880 - f1_m: 0.4830 - val_loss: 1.2494 - val_f1_m: 0.4421\n",
      "Epoch 135/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2905 - f1_m: 0.5024 - val_loss: 1.2481 - val_f1_m: 0.4507\n",
      "Epoch 136/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2892 - f1_m: 0.5007 - val_loss: 1.2470 - val_f1_m: 0.4463\n",
      "Epoch 137/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2880 - f1_m: 0.4864 - val_loss: 1.2456 - val_f1_m: 0.4507\n",
      "Epoch 138/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2841 - f1_m: 0.4795 - val_loss: 1.2442 - val_f1_m: 0.4465\n",
      "Epoch 139/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2778 - f1_m: 0.4822 - val_loss: 1.2431 - val_f1_m: 0.4465\n",
      "Epoch 140/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2817 - f1_m: 0.4910 - val_loss: 1.2422 - val_f1_m: 0.4465\n",
      "Epoch 141/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2838 - f1_m: 0.4942 - val_loss: 1.2410 - val_f1_m: 0.4451\n",
      "Epoch 142/500\n",
      "224/224 [==============================] - 61s 274ms/step - loss: 1.2812 - f1_m: 0.4989 - val_loss: 1.2398 - val_f1_m: 0.4451\n",
      "Epoch 143/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2825 - f1_m: 0.4925 - val_loss: 1.2389 - val_f1_m: 0.4451\n",
      "Epoch 144/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2807 - f1_m: 0.4799 - val_loss: 1.2377 - val_f1_m: 0.4451\n",
      "Epoch 145/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2813 - f1_m: 0.4959 - val_loss: 1.2365 - val_f1_m: 0.4451\n",
      "Epoch 146/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2752 - f1_m: 0.5007 - val_loss: 1.2356 - val_f1_m: 0.4555\n",
      "Epoch 147/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2689 - f1_m: 0.5078 - val_loss: 1.2346 - val_f1_m: 0.4511\n",
      "Epoch 148/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2739 - f1_m: 0.5240 - val_loss: 1.2336 - val_f1_m: 0.4511\n",
      "Epoch 149/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2735 - f1_m: 0.4956 - val_loss: 1.2321 - val_f1_m: 0.4593\n",
      "Epoch 150/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2739 - f1_m: 0.5066 - val_loss: 1.2310 - val_f1_m: 0.4672\n",
      "Epoch 151/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2691 - f1_m: 0.4979 - val_loss: 1.2304 - val_f1_m: 0.4672\n",
      "Epoch 152/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2664 - f1_m: 0.5246 - val_loss: 1.2293 - val_f1_m: 0.4672\n",
      "Epoch 153/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2665 - f1_m: 0.5031 - val_loss: 1.2283 - val_f1_m: 0.4672\n",
      "Epoch 154/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2724 - f1_m: 0.4948 - val_loss: 1.2270 - val_f1_m: 0.4672\n",
      "Epoch 155/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2655 - f1_m: 0.5109 - val_loss: 1.2260 - val_f1_m: 0.4672\n",
      "Epoch 156/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2614 - f1_m: 0.5236 - val_loss: 1.2249 - val_f1_m: 0.4687\n",
      "Epoch 157/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2647 - f1_m: 0.5190 - val_loss: 1.2240 - val_f1_m: 0.4700\n",
      "Epoch 158/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2685 - f1_m: 0.5348 - val_loss: 1.2231 - val_f1_m: 0.4672\n",
      "Epoch 159/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2660 - f1_m: 0.5211 - val_loss: 1.2219 - val_f1_m: 0.4757\n",
      "Epoch 160/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2602 - f1_m: 0.5276 - val_loss: 1.2210 - val_f1_m: 0.4700\n",
      "Epoch 161/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2613 - f1_m: 0.5291 - val_loss: 1.2200 - val_f1_m: 0.4757\n",
      "Epoch 162/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2626 - f1_m: 0.5252 - val_loss: 1.2186 - val_f1_m: 0.4745\n",
      "Epoch 163/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2631 - f1_m: 0.5351 - val_loss: 1.2175 - val_f1_m: 0.4831\n",
      "Epoch 164/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2604 - f1_m: 0.5323 - val_loss: 1.2166 - val_f1_m: 0.4831\n",
      "Epoch 165/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2538 - f1_m: 0.5344 - val_loss: 1.2160 - val_f1_m: 0.4745\n",
      "Epoch 166/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2590 - f1_m: 0.5297 - val_loss: 1.2152 - val_f1_m: 0.4778\n",
      "Epoch 167/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2551 - f1_m: 0.5395 - val_loss: 1.2146 - val_f1_m: 0.4745\n",
      "Epoch 168/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2588 - f1_m: 0.5337 - val_loss: 1.2134 - val_f1_m: 0.4807\n",
      "Epoch 169/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2547 - f1_m: 0.5366 - val_loss: 1.2124 - val_f1_m: 0.4831\n",
      "Epoch 170/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2550 - f1_m: 0.5316 - val_loss: 1.2112 - val_f1_m: 0.4831\n",
      "Epoch 171/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2536 - f1_m: 0.5424 - val_loss: 1.2101 - val_f1_m: 0.4831\n",
      "Epoch 172/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2525 - f1_m: 0.5403 - val_loss: 1.2092 - val_f1_m: 0.4882\n",
      "Epoch 173/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2545 - f1_m: 0.5305 - val_loss: 1.2085 - val_f1_m: 0.4831\n",
      "Epoch 174/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2448 - f1_m: 0.5438 - val_loss: 1.2080 - val_f1_m: 0.4882\n",
      "Epoch 175/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2466 - f1_m: 0.5354 - val_loss: 1.2073 - val_f1_m: 0.4882\n",
      "Epoch 176/500\n",
      "224/224 [==============================] - 57s 253ms/step - loss: 1.2476 - f1_m: 0.5257 - val_loss: 1.2061 - val_f1_m: 0.4983\n",
      "Epoch 177/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2438 - f1_m: 0.5489 - val_loss: 1.2054 - val_f1_m: 0.4983\n",
      "Epoch 178/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2466 - f1_m: 0.5516 - val_loss: 1.2045 - val_f1_m: 0.4983\n",
      "Epoch 179/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2478 - f1_m: 0.5477 - val_loss: 1.2036 - val_f1_m: 0.5015\n",
      "Epoch 180/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2416 - f1_m: 0.5508 - val_loss: 1.2026 - val_f1_m: 0.4983\n",
      "Epoch 181/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2427 - f1_m: 0.5626 - val_loss: 1.2018 - val_f1_m: 0.4972\n",
      "Epoch 182/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2460 - f1_m: 0.5624 - val_loss: 1.2008 - val_f1_m: 0.4972\n",
      "Epoch 183/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2402 - f1_m: 0.5759 - val_loss: 1.2001 - val_f1_m: 0.4972\n",
      "Epoch 184/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2383 - f1_m: 0.5670 - val_loss: 1.1992 - val_f1_m: 0.5019\n",
      "Epoch 185/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2395 - f1_m: 0.5510 - val_loss: 1.1983 - val_f1_m: 0.5052\n",
      "Epoch 186/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2366 - f1_m: 0.5706 - val_loss: 1.1976 - val_f1_m: 0.5090\n",
      "Epoch 187/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2395 - f1_m: 0.5635 - val_loss: 1.1969 - val_f1_m: 0.4972\n",
      "Epoch 188/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2402 - f1_m: 0.5574 - val_loss: 1.1958 - val_f1_m: 0.5080\n",
      "Epoch 189/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2389 - f1_m: 0.5593 - val_loss: 1.1953 - val_f1_m: 0.5124\n",
      "Epoch 190/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2382 - f1_m: 0.5648 - val_loss: 1.1945 - val_f1_m: 0.5124\n",
      "Epoch 191/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2347 - f1_m: 0.5832 - val_loss: 1.1939 - val_f1_m: 0.5124\n",
      "Epoch 192/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2369 - f1_m: 0.5812 - val_loss: 1.1928 - val_f1_m: 0.5220\n",
      "Epoch 193/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2314 - f1_m: 0.5563 - val_loss: 1.1919 - val_f1_m: 0.5220\n",
      "Epoch 194/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2350 - f1_m: 0.5706 - val_loss: 1.1911 - val_f1_m: 0.5220\n",
      "Epoch 195/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2321 - f1_m: 0.5780 - val_loss: 1.1904 - val_f1_m: 0.5220\n",
      "Epoch 196/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2282 - f1_m: 0.5761 - val_loss: 1.1898 - val_f1_m: 0.5187\n",
      "Epoch 197/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2266 - f1_m: 0.5823 - val_loss: 1.1891 - val_f1_m: 0.5220\n",
      "Epoch 198/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2259 - f1_m: 0.5767 - val_loss: 1.1883 - val_f1_m: 0.5249\n",
      "Epoch 199/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2334 - f1_m: 0.5788 - val_loss: 1.1874 - val_f1_m: 0.5249\n",
      "Epoch 200/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2301 - f1_m: 0.5777 - val_loss: 1.1867 - val_f1_m: 0.5303\n",
      "Epoch 201/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2243 - f1_m: 0.5797 - val_loss: 1.1858 - val_f1_m: 0.5303\n",
      "Epoch 202/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2309 - f1_m: 0.5713 - val_loss: 1.1850 - val_f1_m: 0.5303\n",
      "Epoch 203/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2296 - f1_m: 0.5781 - val_loss: 1.1839 - val_f1_m: 0.5303\n",
      "Epoch 204/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2150 - f1_m: 0.5945 - val_loss: 1.1833 - val_f1_m: 0.5303\n",
      "Epoch 205/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2216 - f1_m: 0.5895 - val_loss: 1.1825 - val_f1_m: 0.5303\n",
      "Epoch 206/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2247 - f1_m: 0.6020 - val_loss: 1.1819 - val_f1_m: 0.5303\n",
      "Epoch 207/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2225 - f1_m: 0.5932 - val_loss: 1.1812 - val_f1_m: 0.5303\n",
      "Epoch 208/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2175 - f1_m: 0.5815 - val_loss: 1.1806 - val_f1_m: 0.5303\n",
      "Epoch 209/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2193 - f1_m: 0.5855 - val_loss: 1.1799 - val_f1_m: 0.5303\n",
      "Epoch 210/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2244 - f1_m: 0.5871 - val_loss: 1.1787 - val_f1_m: 0.5303\n",
      "Epoch 211/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2198 - f1_m: 0.6046 - val_loss: 1.1781 - val_f1_m: 0.5344\n",
      "Epoch 212/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2193 - f1_m: 0.6047 - val_loss: 1.1776 - val_f1_m: 0.5336\n",
      "Epoch 213/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2210 - f1_m: 0.5975 - val_loss: 1.1766 - val_f1_m: 0.5410\n",
      "Epoch 214/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2178 - f1_m: 0.6052 - val_loss: 1.1757 - val_f1_m: 0.5410\n",
      "Epoch 215/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2139 - f1_m: 0.6099 - val_loss: 1.1749 - val_f1_m: 0.5410\n",
      "Epoch 216/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2157 - f1_m: 0.6072 - val_loss: 1.1742 - val_f1_m: 0.5410\n",
      "Epoch 217/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2163 - f1_m: 0.6012 - val_loss: 1.1736 - val_f1_m: 0.5410\n",
      "Epoch 218/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2132 - f1_m: 0.5984 - val_loss: 1.1729 - val_f1_m: 0.5441\n",
      "Epoch 219/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2112 - f1_m: 0.6034 - val_loss: 1.1727 - val_f1_m: 0.5370\n",
      "Epoch 220/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2110 - f1_m: 0.6041 - val_loss: 1.1716 - val_f1_m: 0.5441\n",
      "Epoch 221/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2097 - f1_m: 0.5832 - val_loss: 1.1709 - val_f1_m: 0.5441\n",
      "Epoch 222/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2071 - f1_m: 0.6031 - val_loss: 1.1699 - val_f1_m: 0.5433\n",
      "Epoch 223/500\n",
      "224/224 [==============================] - 56s 249ms/step - loss: 1.2034 - f1_m: 0.6099 - val_loss: 1.1694 - val_f1_m: 0.5485\n",
      "Epoch 224/500\n",
      "224/224 [==============================] - 56s 250ms/step - loss: 1.2057 - f1_m: 0.6116 - val_loss: 1.1688 - val_f1_m: 0.5466\n",
      "Epoch 225/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2024 - f1_m: 0.6114 - val_loss: 1.1684 - val_f1_m: 0.5473\n",
      "Epoch 226/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2086 - f1_m: 0.5987 - val_loss: 1.1676 - val_f1_m: 0.5542\n",
      "Epoch 227/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2130 - f1_m: 0.6230 - val_loss: 1.1670 - val_f1_m: 0.5532\n",
      "Epoch 228/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2027 - f1_m: 0.6100 - val_loss: 1.1663 - val_f1_m: 0.5524\n",
      "Epoch 229/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2023 - f1_m: 0.6039 - val_loss: 1.1656 - val_f1_m: 0.5532\n",
      "Epoch 230/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1999 - f1_m: 0.6197 - val_loss: 1.1648 - val_f1_m: 0.5499\n",
      "Epoch 231/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2110 - f1_m: 0.6102 - val_loss: 1.1641 - val_f1_m: 0.5526\n",
      "Epoch 232/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1995 - f1_m: 0.6080 - val_loss: 1.1633 - val_f1_m: 0.5513\n",
      "Epoch 233/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2006 - f1_m: 0.6205 - val_loss: 1.1629 - val_f1_m: 0.5509\n",
      "Epoch 234/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2020 - f1_m: 0.6089 - val_loss: 1.1624 - val_f1_m: 0.5509\n",
      "Epoch 235/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2015 - f1_m: 0.6205 - val_loss: 1.1619 - val_f1_m: 0.5509\n",
      "Epoch 236/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2045 - f1_m: 0.6284 - val_loss: 1.1610 - val_f1_m: 0.5535\n",
      "Epoch 237/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1962 - f1_m: 0.6244 - val_loss: 1.1603 - val_f1_m: 0.5535\n",
      "Epoch 238/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1978 - f1_m: 0.6301 - val_loss: 1.1596 - val_f1_m: 0.5555\n",
      "Epoch 239/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2003 - f1_m: 0.6157 - val_loss: 1.1591 - val_f1_m: 0.5556\n",
      "Epoch 240/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1951 - f1_m: 0.6306 - val_loss: 1.1587 - val_f1_m: 0.5555\n",
      "Epoch 241/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1937 - f1_m: 0.6360 - val_loss: 1.1579 - val_f1_m: 0.5555\n",
      "Epoch 242/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.2006 - f1_m: 0.6369 - val_loss: 1.1572 - val_f1_m: 0.5555\n",
      "Epoch 243/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.2025 - f1_m: 0.6292 - val_loss: 1.1566 - val_f1_m: 0.5570\n",
      "Epoch 244/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1961 - f1_m: 0.6312 - val_loss: 1.1558 - val_f1_m: 0.5555\n",
      "Epoch 245/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1986 - f1_m: 0.6258 - val_loss: 1.1552 - val_f1_m: 0.5570\n",
      "Epoch 246/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1967 - f1_m: 0.6406 - val_loss: 1.1546 - val_f1_m: 0.5555\n",
      "Epoch 247/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1940 - f1_m: 0.6266 - val_loss: 1.1541 - val_f1_m: 0.5510\n",
      "Epoch 248/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1929 - f1_m: 0.6397 - val_loss: 1.1533 - val_f1_m: 0.5469\n",
      "Epoch 249/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1977 - f1_m: 0.6291 - val_loss: 1.1527 - val_f1_m: 0.5496\n",
      "Epoch 250/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1925 - f1_m: 0.6295 - val_loss: 1.1520 - val_f1_m: 0.5496\n",
      "Epoch 251/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1857 - f1_m: 0.6464 - val_loss: 1.1514 - val_f1_m: 0.5496\n",
      "Epoch 252/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1947 - f1_m: 0.6548 - val_loss: 1.1510 - val_f1_m: 0.5496\n",
      "Epoch 253/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1874 - f1_m: 0.6477 - val_loss: 1.1504 - val_f1_m: 0.5496\n",
      "Epoch 254/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1905 - f1_m: 0.6416 - val_loss: 1.1498 - val_f1_m: 0.5560\n",
      "Epoch 255/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1908 - f1_m: 0.6438 - val_loss: 1.1492 - val_f1_m: 0.5496\n",
      "Epoch 256/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1892 - f1_m: 0.6511 - val_loss: 1.1485 - val_f1_m: 0.5604\n",
      "Epoch 257/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1829 - f1_m: 0.6458 - val_loss: 1.1478 - val_f1_m: 0.5571\n",
      "Epoch 258/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1876 - f1_m: 0.6300 - val_loss: 1.1473 - val_f1_m: 0.5625\n",
      "Epoch 259/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1832 - f1_m: 0.6589 - val_loss: 1.1465 - val_f1_m: 0.5615\n",
      "Epoch 260/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1853 - f1_m: 0.6426 - val_loss: 1.1462 - val_f1_m: 0.5594\n",
      "Epoch 261/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1827 - f1_m: 0.6467 - val_loss: 1.1456 - val_f1_m: 0.5561\n",
      "Epoch 262/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1800 - f1_m: 0.6469 - val_loss: 1.1450 - val_f1_m: 0.5691\n",
      "Epoch 263/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1793 - f1_m: 0.6471 - val_loss: 1.1444 - val_f1_m: 0.5714\n",
      "Epoch 264/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1910 - f1_m: 0.6523 - val_loss: 1.1440 - val_f1_m: 0.5686\n",
      "Epoch 265/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1804 - f1_m: 0.6576 - val_loss: 1.1436 - val_f1_m: 0.5693\n",
      "Epoch 266/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1776 - f1_m: 0.6630 - val_loss: 1.1430 - val_f1_m: 0.5686\n",
      "Epoch 267/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1814 - f1_m: 0.6528 - val_loss: 1.1424 - val_f1_m: 0.5714\n",
      "Epoch 268/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1880 - f1_m: 0.6483 - val_loss: 1.1418 - val_f1_m: 0.5714\n",
      "Epoch 269/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1741 - f1_m: 0.6486 - val_loss: 1.1413 - val_f1_m: 0.5714\n",
      "Epoch 270/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1867 - f1_m: 0.6643 - val_loss: 1.1408 - val_f1_m: 0.5693\n",
      "Epoch 271/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1801 - f1_m: 0.6547 - val_loss: 1.1399 - val_f1_m: 0.5734\n",
      "Epoch 272/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1811 - f1_m: 0.6475 - val_loss: 1.1391 - val_f1_m: 0.5679\n",
      "Epoch 273/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1781 - f1_m: 0.6466 - val_loss: 1.1388 - val_f1_m: 0.5760\n",
      "Epoch 274/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1772 - f1_m: 0.6571 - val_loss: 1.1384 - val_f1_m: 0.5750\n",
      "Epoch 275/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1763 - f1_m: 0.6690 - val_loss: 1.1379 - val_f1_m: 0.5760\n",
      "Epoch 276/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1783 - f1_m: 0.6601 - val_loss: 1.1374 - val_f1_m: 0.5791\n",
      "Epoch 277/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1876 - f1_m: 0.6591 - val_loss: 1.1368 - val_f1_m: 0.5791\n",
      "Epoch 278/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1802 - f1_m: 0.6657 - val_loss: 1.1360 - val_f1_m: 0.5732\n",
      "Epoch 279/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1786 - f1_m: 0.6621 - val_loss: 1.1356 - val_f1_m: 0.5740\n",
      "Epoch 280/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1777 - f1_m: 0.6575 - val_loss: 1.1351 - val_f1_m: 0.5783\n",
      "Epoch 281/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1794 - f1_m: 0.6547 - val_loss: 1.1345 - val_f1_m: 0.5754\n",
      "Epoch 282/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1714 - f1_m: 0.6846 - val_loss: 1.1340 - val_f1_m: 0.5744\n",
      "Epoch 283/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1755 - f1_m: 0.6634 - val_loss: 1.1334 - val_f1_m: 0.5734\n",
      "Epoch 284/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1733 - f1_m: 0.6525 - val_loss: 1.1329 - val_f1_m: 0.5734\n",
      "Epoch 285/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1702 - f1_m: 0.6719 - val_loss: 1.1322 - val_f1_m: 0.5813\n",
      "Epoch 286/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1704 - f1_m: 0.6916 - val_loss: 1.1319 - val_f1_m: 0.5800\n",
      "Epoch 287/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1689 - f1_m: 0.6631 - val_loss: 1.1317 - val_f1_m: 0.5870\n",
      "Epoch 288/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1715 - f1_m: 0.6755 - val_loss: 1.1312 - val_f1_m: 0.5813\n",
      "Epoch 289/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1797 - f1_m: 0.6812 - val_loss: 1.1307 - val_f1_m: 0.5841\n",
      "Epoch 290/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1748 - f1_m: 0.6650 - val_loss: 1.1300 - val_f1_m: 0.5861\n",
      "Epoch 291/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1792 - f1_m: 0.6567 - val_loss: 1.1294 - val_f1_m: 0.5867\n",
      "Epoch 292/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1616 - f1_m: 0.6801 - val_loss: 1.1290 - val_f1_m: 0.5895\n",
      "Epoch 293/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1672 - f1_m: 0.6710 - val_loss: 1.1285 - val_f1_m: 0.5906\n",
      "Epoch 294/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1666 - f1_m: 0.6689 - val_loss: 1.1278 - val_f1_m: 0.5974\n",
      "Epoch 295/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1605 - f1_m: 0.6740 - val_loss: 1.1273 - val_f1_m: 0.5989\n",
      "Epoch 296/500\n",
      "224/224 [==============================] - 56s 249ms/step - loss: 1.1638 - f1_m: 0.6802 - val_loss: 1.1270 - val_f1_m: 0.6010\n",
      "Epoch 297/500\n",
      "224/224 [==============================] - 56s 250ms/step - loss: 1.1673 - f1_m: 0.6684 - val_loss: 1.1264 - val_f1_m: 0.5976\n",
      "Epoch 298/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1629 - f1_m: 0.6876 - val_loss: 1.1262 - val_f1_m: 0.5876\n",
      "Epoch 299/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1736 - f1_m: 0.6715 - val_loss: 1.1256 - val_f1_m: 0.6004\n",
      "Epoch 300/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1617 - f1_m: 0.6741 - val_loss: 1.1249 - val_f1_m: 0.5924\n",
      "Epoch 301/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1697 - f1_m: 0.6724 - val_loss: 1.1246 - val_f1_m: 0.6003\n",
      "Epoch 302/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1646 - f1_m: 0.6762 - val_loss: 1.1239 - val_f1_m: 0.6003\n",
      "Epoch 303/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1620 - f1_m: 0.6734 - val_loss: 1.1236 - val_f1_m: 0.5993\n",
      "Epoch 304/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1725 - f1_m: 0.6820 - val_loss: 1.1232 - val_f1_m: 0.6052\n",
      "Epoch 305/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1678 - f1_m: 0.6797 - val_loss: 1.1227 - val_f1_m: 0.5981\n",
      "Epoch 306/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1604 - f1_m: 0.6900 - val_loss: 1.1222 - val_f1_m: 0.6003\n",
      "Epoch 307/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1629 - f1_m: 0.6841 - val_loss: 1.1220 - val_f1_m: 0.5960\n",
      "Epoch 308/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1657 - f1_m: 0.6810 - val_loss: 1.1213 - val_f1_m: 0.6052\n",
      "Epoch 309/500\n",
      "224/224 [==============================] - 56s 248ms/step - loss: 1.1572 - f1_m: 0.6979 - val_loss: 1.1209 - val_f1_m: 0.6043\n",
      "Epoch 310/500\n",
      "224/224 [==============================] - 56s 250ms/step - loss: 1.1550 - f1_m: 0.6942 - val_loss: 1.1205 - val_f1_m: 0.6031\n",
      "Epoch 311/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1588 - f1_m: 0.6925 - val_loss: 1.1201 - val_f1_m: 0.6043\n",
      "Epoch 312/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1566 - f1_m: 0.6843 - val_loss: 1.1200 - val_f1_m: 0.5981\n",
      "Epoch 313/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1593 - f1_m: 0.6781 - val_loss: 1.1191 - val_f1_m: 0.6031\n",
      "Epoch 314/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1551 - f1_m: 0.6798 - val_loss: 1.1187 - val_f1_m: 0.6043\n",
      "Epoch 315/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1546 - f1_m: 0.6908 - val_loss: 1.1184 - val_f1_m: 0.6031\n",
      "Epoch 316/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1546 - f1_m: 0.6984 - val_loss: 1.1176 - val_f1_m: 0.6014\n",
      "Epoch 317/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1579 - f1_m: 0.6820 - val_loss: 1.1169 - val_f1_m: 0.6052\n",
      "Epoch 318/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1547 - f1_m: 0.7122 - val_loss: 1.1162 - val_f1_m: 0.6074\n",
      "Epoch 319/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1588 - f1_m: 0.6988 - val_loss: 1.1157 - val_f1_m: 0.6099\n",
      "Epoch 320/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1529 - f1_m: 0.7026 - val_loss: 1.1156 - val_f1_m: 0.6014\n",
      "Epoch 321/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1461 - f1_m: 0.7055 - val_loss: 1.1151 - val_f1_m: 0.6070\n",
      "Epoch 322/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1496 - f1_m: 0.7003 - val_loss: 1.1146 - val_f1_m: 0.6072\n",
      "Epoch 323/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1519 - f1_m: 0.6991 - val_loss: 1.1142 - val_f1_m: 0.6104\n",
      "Epoch 324/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1486 - f1_m: 0.6961 - val_loss: 1.1136 - val_f1_m: 0.6117\n",
      "Epoch 325/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1499 - f1_m: 0.6959 - val_loss: 1.1132 - val_f1_m: 0.6176\n",
      "Epoch 326/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1506 - f1_m: 0.6920 - val_loss: 1.1128 - val_f1_m: 0.6119\n",
      "Epoch 327/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1496 - f1_m: 0.6952 - val_loss: 1.1125 - val_f1_m: 0.6177\n",
      "Epoch 328/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1551 - f1_m: 0.7071 - val_loss: 1.1120 - val_f1_m: 0.6183\n",
      "Epoch 329/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1492 - f1_m: 0.7171 - val_loss: 1.1115 - val_f1_m: 0.6133\n",
      "Epoch 330/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1495 - f1_m: 0.7145 - val_loss: 1.1110 - val_f1_m: 0.6159\n",
      "Epoch 331/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1489 - f1_m: 0.6988 - val_loss: 1.1104 - val_f1_m: 0.6187\n",
      "Epoch 332/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1469 - f1_m: 0.7212 - val_loss: 1.1103 - val_f1_m: 0.6144\n",
      "Epoch 333/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1423 - f1_m: 0.7018 - val_loss: 1.1098 - val_f1_m: 0.6197\n",
      "Epoch 334/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1456 - f1_m: 0.6886 - val_loss: 1.1093 - val_f1_m: 0.6259\n",
      "Epoch 335/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1455 - f1_m: 0.7031 - val_loss: 1.1087 - val_f1_m: 0.6266\n",
      "Epoch 336/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1497 - f1_m: 0.7054 - val_loss: 1.1085 - val_f1_m: 0.6236\n",
      "Epoch 337/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1490 - f1_m: 0.7046 - val_loss: 1.1081 - val_f1_m: 0.6221\n",
      "Epoch 338/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1445 - f1_m: 0.7137 - val_loss: 1.1076 - val_f1_m: 0.6280\n",
      "Epoch 339/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1455 - f1_m: 0.6968 - val_loss: 1.1073 - val_f1_m: 0.6323\n",
      "Epoch 340/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1469 - f1_m: 0.7165 - val_loss: 1.1066 - val_f1_m: 0.6270\n",
      "Epoch 341/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1503 - f1_m: 0.7122 - val_loss: 1.1060 - val_f1_m: 0.6353\n",
      "Epoch 342/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1515 - f1_m: 0.7029 - val_loss: 1.1056 - val_f1_m: 0.6372\n",
      "Epoch 343/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1486 - f1_m: 0.7159 - val_loss: 1.1052 - val_f1_m: 0.6291\n",
      "Epoch 344/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1441 - f1_m: 0.7052 - val_loss: 1.1050 - val_f1_m: 0.6248\n",
      "Epoch 345/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1524 - f1_m: 0.7067 - val_loss: 1.1042 - val_f1_m: 0.6358\n",
      "Epoch 346/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1486 - f1_m: 0.7274 - val_loss: 1.1041 - val_f1_m: 0.6271\n",
      "Epoch 347/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1442 - f1_m: 0.7139 - val_loss: 1.1038 - val_f1_m: 0.6291\n",
      "Epoch 348/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1393 - f1_m: 0.7072 - val_loss: 1.1032 - val_f1_m: 0.6315\n",
      "Epoch 349/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1403 - f1_m: 0.7197 - val_loss: 1.1031 - val_f1_m: 0.6294\n",
      "Epoch 350/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1470 - f1_m: 0.7289 - val_loss: 1.1026 - val_f1_m: 0.6369\n",
      "Epoch 351/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1471 - f1_m: 0.7117 - val_loss: 1.1021 - val_f1_m: 0.6365\n",
      "Epoch 352/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1423 - f1_m: 0.7119 - val_loss: 1.1017 - val_f1_m: 0.6387\n",
      "Epoch 353/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1370 - f1_m: 0.7170 - val_loss: 1.1015 - val_f1_m: 0.6476\n",
      "Epoch 354/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1440 - f1_m: 0.7020 - val_loss: 1.1008 - val_f1_m: 0.6481\n",
      "Epoch 355/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1356 - f1_m: 0.7234 - val_loss: 1.1002 - val_f1_m: 0.6499\n",
      "Epoch 356/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1408 - f1_m: 0.7084 - val_loss: 1.0999 - val_f1_m: 0.6519\n",
      "Epoch 357/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1349 - f1_m: 0.7264 - val_loss: 1.0997 - val_f1_m: 0.6453\n",
      "Epoch 358/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1366 - f1_m: 0.7197 - val_loss: 1.0994 - val_f1_m: 0.6440\n",
      "Epoch 359/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1386 - f1_m: 0.7269 - val_loss: 1.0987 - val_f1_m: 0.6453\n",
      "Epoch 360/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1364 - f1_m: 0.7235 - val_loss: 1.0986 - val_f1_m: 0.6434\n",
      "Epoch 361/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1376 - f1_m: 0.7294 - val_loss: 1.0982 - val_f1_m: 0.6457\n",
      "Epoch 362/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1439 - f1_m: 0.7312 - val_loss: 1.0978 - val_f1_m: 0.6477\n",
      "Epoch 363/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1298 - f1_m: 0.7322 - val_loss: 1.0975 - val_f1_m: 0.6456\n",
      "Epoch 364/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1392 - f1_m: 0.7305 - val_loss: 1.0969 - val_f1_m: 0.6506\n",
      "Epoch 365/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1347 - f1_m: 0.7224 - val_loss: 1.0965 - val_f1_m: 0.6569\n",
      "Epoch 366/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1399 - f1_m: 0.7171 - val_loss: 1.0961 - val_f1_m: 0.6571\n",
      "Epoch 367/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1341 - f1_m: 0.7082 - val_loss: 1.0957 - val_f1_m: 0.6559\n",
      "Epoch 368/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1321 - f1_m: 0.7407 - val_loss: 1.0952 - val_f1_m: 0.6556\n",
      "Epoch 369/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1276 - f1_m: 0.7352 - val_loss: 1.0948 - val_f1_m: 0.6590\n",
      "Epoch 370/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1314 - f1_m: 0.7225 - val_loss: 1.0945 - val_f1_m: 0.6581\n",
      "Epoch 371/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1264 - f1_m: 0.7255 - val_loss: 1.0941 - val_f1_m: 0.6713\n",
      "Epoch 372/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1336 - f1_m: 0.7199 - val_loss: 1.0937 - val_f1_m: 0.6670\n",
      "Epoch 373/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1300 - f1_m: 0.7258 - val_loss: 1.0932 - val_f1_m: 0.6691\n",
      "Epoch 374/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1302 - f1_m: 0.7335 - val_loss: 1.0929 - val_f1_m: 0.6738\n",
      "Epoch 375/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1317 - f1_m: 0.7358 - val_loss: 1.0922 - val_f1_m: 0.6746\n",
      "Epoch 376/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1316 - f1_m: 0.7345 - val_loss: 1.0920 - val_f1_m: 0.6661\n",
      "Epoch 377/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1360 - f1_m: 0.7194 - val_loss: 1.0917 - val_f1_m: 0.6690\n",
      "Epoch 378/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1290 - f1_m: 0.7308 - val_loss: 1.0916 - val_f1_m: 0.6660\n",
      "Epoch 379/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1337 - f1_m: 0.7300 - val_loss: 1.0911 - val_f1_m: 0.6686\n",
      "Epoch 380/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1245 - f1_m: 0.7428 - val_loss: 1.0907 - val_f1_m: 0.6769\n",
      "Epoch 381/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1390 - f1_m: 0.7262 - val_loss: 1.0903 - val_f1_m: 0.6728\n",
      "Epoch 382/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1321 - f1_m: 0.7320 - val_loss: 1.0898 - val_f1_m: 0.6769\n",
      "Epoch 383/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1291 - f1_m: 0.7454 - val_loss: 1.0895 - val_f1_m: 0.6790\n",
      "Epoch 384/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1265 - f1_m: 0.7226 - val_loss: 1.0891 - val_f1_m: 0.6723\n",
      "Epoch 385/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1263 - f1_m: 0.7359 - val_loss: 1.0888 - val_f1_m: 0.6777\n",
      "Epoch 386/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1320 - f1_m: 0.7352 - val_loss: 1.0884 - val_f1_m: 0.6812\n",
      "Epoch 387/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1276 - f1_m: 0.7333 - val_loss: 1.0882 - val_f1_m: 0.6758\n",
      "Epoch 388/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1216 - f1_m: 0.7372 - val_loss: 1.0880 - val_f1_m: 0.6706\n",
      "Epoch 389/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1315 - f1_m: 0.7281 - val_loss: 1.0872 - val_f1_m: 0.6775\n",
      "Epoch 390/500\n",
      "224/224 [==============================] - 57s 252ms/step - loss: 1.1267 - f1_m: 0.7245 - val_loss: 1.0867 - val_f1_m: 0.6798\n",
      "Epoch 391/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1233 - f1_m: 0.7391 - val_loss: 1.0864 - val_f1_m: 0.6827\n",
      "Epoch 392/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1271 - f1_m: 0.7340 - val_loss: 1.0860 - val_f1_m: 0.6779\n",
      "Epoch 393/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1262 - f1_m: 0.7294 - val_loss: 1.0856 - val_f1_m: 0.6784\n",
      "Epoch 394/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1218 - f1_m: 0.7385 - val_loss: 1.0853 - val_f1_m: 0.6827\n",
      "Epoch 395/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1261 - f1_m: 0.7471 - val_loss: 1.0851 - val_f1_m: 0.6784\n",
      "Epoch 396/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1274 - f1_m: 0.7356 - val_loss: 1.0846 - val_f1_m: 0.6826\n",
      "Epoch 397/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1259 - f1_m: 0.7334 - val_loss: 1.0843 - val_f1_m: 0.6804\n",
      "Epoch 398/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1252 - f1_m: 0.7407 - val_loss: 1.0838 - val_f1_m: 0.6783\n",
      "Epoch 399/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1241 - f1_m: 0.7206 - val_loss: 1.0833 - val_f1_m: 0.6825\n",
      "Epoch 400/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1168 - f1_m: 0.7229 - val_loss: 1.0828 - val_f1_m: 0.6911\n",
      "Epoch 401/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1216 - f1_m: 0.7347 - val_loss: 1.0827 - val_f1_m: 0.6849\n",
      "Epoch 402/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1248 - f1_m: 0.7381 - val_loss: 1.0824 - val_f1_m: 0.6887\n",
      "Epoch 403/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1223 - f1_m: 0.7421 - val_loss: 1.0821 - val_f1_m: 0.6850\n",
      "Epoch 404/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1203 - f1_m: 0.7490 - val_loss: 1.0817 - val_f1_m: 0.6845\n",
      "Epoch 405/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1246 - f1_m: 0.7361 - val_loss: 1.0812 - val_f1_m: 0.6845\n",
      "Epoch 406/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1211 - f1_m: 0.7421 - val_loss: 1.0809 - val_f1_m: 0.6871\n",
      "Epoch 407/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1270 - f1_m: 0.7454 - val_loss: 1.0804 - val_f1_m: 0.6911\n",
      "Epoch 408/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1199 - f1_m: 0.7404 - val_loss: 1.0800 - val_f1_m: 0.6937\n",
      "Epoch 409/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1135 - f1_m: 0.7457 - val_loss: 1.0796 - val_f1_m: 0.6911\n",
      "Epoch 410/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1175 - f1_m: 0.7431 - val_loss: 1.0794 - val_f1_m: 0.6940\n",
      "Epoch 411/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1151 - f1_m: 0.7347 - val_loss: 1.0792 - val_f1_m: 0.6937\n",
      "Epoch 412/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1231 - f1_m: 0.7345 - val_loss: 1.0789 - val_f1_m: 0.6911\n",
      "Epoch 413/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1144 - f1_m: 0.7391 - val_loss: 1.0788 - val_f1_m: 0.6911\n",
      "Epoch 414/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1243 - f1_m: 0.7426 - val_loss: 1.0786 - val_f1_m: 0.6914\n",
      "Epoch 415/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1187 - f1_m: 0.7379 - val_loss: 1.0783 - val_f1_m: 0.6890\n",
      "Epoch 416/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1200 - f1_m: 0.7271 - val_loss: 1.0779 - val_f1_m: 0.6969\n",
      "Epoch 417/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1232 - f1_m: 0.7329 - val_loss: 1.0774 - val_f1_m: 0.6898\n",
      "Epoch 418/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1224 - f1_m: 0.7408 - val_loss: 1.0773 - val_f1_m: 0.6846\n",
      "Epoch 419/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1210 - f1_m: 0.7517 - val_loss: 1.0771 - val_f1_m: 0.6901\n",
      "Epoch 420/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1173 - f1_m: 0.7525 - val_loss: 1.0765 - val_f1_m: 0.6945\n",
      "Epoch 421/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1150 - f1_m: 0.7430 - val_loss: 1.0764 - val_f1_m: 0.6885\n",
      "Epoch 422/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1145 - f1_m: 0.7526 - val_loss: 1.0757 - val_f1_m: 0.6940\n",
      "Epoch 423/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1163 - f1_m: 0.7584 - val_loss: 1.0754 - val_f1_m: 0.6904\n",
      "Epoch 424/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1101 - f1_m: 0.7503 - val_loss: 1.0751 - val_f1_m: 0.6923\n",
      "Epoch 425/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1132 - f1_m: 0.7508 - val_loss: 1.0748 - val_f1_m: 0.6923\n",
      "Epoch 426/500\n",
      "224/224 [==============================] - 57s 252ms/step - loss: 1.1106 - f1_m: 0.7580 - val_loss: 1.0743 - val_f1_m: 0.6911\n",
      "Epoch 427/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1108 - f1_m: 0.7512 - val_loss: 1.0739 - val_f1_m: 0.6911\n",
      "Epoch 428/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1174 - f1_m: 0.7507 - val_loss: 1.0736 - val_f1_m: 0.6967\n",
      "Epoch 429/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1136 - f1_m: 0.7499 - val_loss: 1.0735 - val_f1_m: 0.6908\n",
      "Epoch 430/500\n",
      "224/224 [==============================] - 57s 252ms/step - loss: 1.1073 - f1_m: 0.7601 - val_loss: 1.0731 - val_f1_m: 0.6931\n",
      "Epoch 431/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1143 - f1_m: 0.7655 - val_loss: 1.0727 - val_f1_m: 0.6983\n",
      "Epoch 432/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1140 - f1_m: 0.7522 - val_loss: 1.0721 - val_f1_m: 0.7010\n",
      "Epoch 433/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1119 - f1_m: 0.7564 - val_loss: 1.0719 - val_f1_m: 0.7040\n",
      "Epoch 434/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1120 - f1_m: 0.7543 - val_loss: 1.0717 - val_f1_m: 0.7083\n",
      "Epoch 435/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1145 - f1_m: 0.7624 - val_loss: 1.0715 - val_f1_m: 0.7069\n",
      "Epoch 436/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1065 - f1_m: 0.7603 - val_loss: 1.0711 - val_f1_m: 0.7111\n",
      "Epoch 437/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1069 - f1_m: 0.7423 - val_loss: 1.0706 - val_f1_m: 0.7139\n",
      "Epoch 438/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1097 - f1_m: 0.7490 - val_loss: 1.0703 - val_f1_m: 0.7091\n",
      "Epoch 439/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1112 - f1_m: 0.7637 - val_loss: 1.0699 - val_f1_m: 0.7053\n",
      "Epoch 440/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1072 - f1_m: 0.7796 - val_loss: 1.0698 - val_f1_m: 0.7079\n",
      "Epoch 441/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1088 - f1_m: 0.7541 - val_loss: 1.0694 - val_f1_m: 0.7129\n",
      "Epoch 442/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1026 - f1_m: 0.7700 - val_loss: 1.0693 - val_f1_m: 0.7134\n",
      "Epoch 443/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1126 - f1_m: 0.7850 - val_loss: 1.0687 - val_f1_m: 0.7138\n",
      "Epoch 444/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1051 - f1_m: 0.7715 - val_loss: 1.0684 - val_f1_m: 0.7109\n",
      "Epoch 445/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1070 - f1_m: 0.7552 - val_loss: 1.0682 - val_f1_m: 0.7109\n",
      "Epoch 446/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1045 - f1_m: 0.7514 - val_loss: 1.0678 - val_f1_m: 0.7138\n",
      "Epoch 447/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1062 - f1_m: 0.7574 - val_loss: 1.0673 - val_f1_m: 0.7143\n",
      "Epoch 448/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1073 - f1_m: 0.7616 - val_loss: 1.0671 - val_f1_m: 0.7096\n",
      "Epoch 449/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1085 - f1_m: 0.7603 - val_loss: 1.0668 - val_f1_m: 0.7164\n",
      "Epoch 450/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1113 - f1_m: 0.7518 - val_loss: 1.0664 - val_f1_m: 0.7209\n",
      "Epoch 451/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1059 - f1_m: 0.7770 - val_loss: 1.0663 - val_f1_m: 0.7186\n",
      "Epoch 452/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1012 - f1_m: 0.7672 - val_loss: 1.0659 - val_f1_m: 0.7164\n",
      "Epoch 453/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1040 - f1_m: 0.7757 - val_loss: 1.0655 - val_f1_m: 0.7188\n",
      "Epoch 454/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1010 - f1_m: 0.7690 - val_loss: 1.0654 - val_f1_m: 0.7188\n",
      "Epoch 455/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1095 - f1_m: 0.7644 - val_loss: 1.0652 - val_f1_m: 0.7164\n",
      "Epoch 456/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0986 - f1_m: 0.7705 - val_loss: 1.0648 - val_f1_m: 0.7156\n",
      "Epoch 457/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1100 - f1_m: 0.7666 - val_loss: 1.0642 - val_f1_m: 0.7204\n",
      "Epoch 458/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0982 - f1_m: 0.7640 - val_loss: 1.0639 - val_f1_m: 0.7209\n",
      "Epoch 459/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1040 - f1_m: 0.7725 - val_loss: 1.0637 - val_f1_m: 0.7209\n",
      "Epoch 460/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1075 - f1_m: 0.7646 - val_loss: 1.0634 - val_f1_m: 0.7172\n",
      "Epoch 461/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1069 - f1_m: 0.7529 - val_loss: 1.0629 - val_f1_m: 0.7204\n",
      "Epoch 462/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0976 - f1_m: 0.7635 - val_loss: 1.0629 - val_f1_m: 0.7229\n",
      "Epoch 463/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1027 - f1_m: 0.7667 - val_loss: 1.0627 - val_f1_m: 0.7223\n",
      "Epoch 464/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1055 - f1_m: 0.7654 - val_loss: 1.0624 - val_f1_m: 0.7231\n",
      "Epoch 465/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1002 - f1_m: 0.7630 - val_loss: 1.0620 - val_f1_m: 0.7204\n",
      "Epoch 466/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0920 - f1_m: 0.7593 - val_loss: 1.0619 - val_f1_m: 0.7253\n",
      "Epoch 467/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.0934 - f1_m: 0.7586 - val_loss: 1.0616 - val_f1_m: 0.7231\n",
      "Epoch 468/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1030 - f1_m: 0.7756 - val_loss: 1.0613 - val_f1_m: 0.7228\n",
      "Epoch 469/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1013 - f1_m: 0.7780 - val_loss: 1.0611 - val_f1_m: 0.7250\n",
      "Epoch 470/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.0970 - f1_m: 0.7643 - val_loss: 1.0609 - val_f1_m: 0.7210\n",
      "Epoch 471/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1086 - f1_m: 0.7593 - val_loss: 1.0604 - val_f1_m: 0.7253\n",
      "Epoch 472/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1045 - f1_m: 0.7612 - val_loss: 1.0601 - val_f1_m: 0.7231\n",
      "Epoch 473/500\n",
      "224/224 [==============================] - 56s 249ms/step - loss: 1.0958 - f1_m: 0.7668 - val_loss: 1.0597 - val_f1_m: 0.7253\n",
      "Epoch 474/500\n",
      "224/224 [==============================] - 56s 250ms/step - loss: 1.0961 - f1_m: 0.7643 - val_loss: 1.0593 - val_f1_m: 0.7250\n",
      "Epoch 475/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1038 - f1_m: 0.7845 - val_loss: 1.0592 - val_f1_m: 0.7272\n",
      "Epoch 476/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.1065 - f1_m: 0.7777 - val_loss: 1.0590 - val_f1_m: 0.7292\n",
      "Epoch 477/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0975 - f1_m: 0.7672 - val_loss: 1.0586 - val_f1_m: 0.7314\n",
      "Epoch 478/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1024 - f1_m: 0.7746 - val_loss: 1.0583 - val_f1_m: 0.7314\n",
      "Epoch 479/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.0999 - f1_m: 0.7662 - val_loss: 1.0581 - val_f1_m: 0.7357\n",
      "Epoch 480/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.0951 - f1_m: 0.7676 - val_loss: 1.0578 - val_f1_m: 0.7314\n",
      "Epoch 481/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0967 - f1_m: 0.7730 - val_loss: 1.0575 - val_f1_m: 0.7314\n",
      "Epoch 482/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.1066 - f1_m: 0.7569 - val_loss: 1.0572 - val_f1_m: 0.7295\n",
      "Epoch 483/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0938 - f1_m: 0.7702 - val_loss: 1.0567 - val_f1_m: 0.7333\n",
      "Epoch 484/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0906 - f1_m: 0.7623 - val_loss: 1.0564 - val_f1_m: 0.7333\n",
      "Epoch 485/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0901 - f1_m: 0.7853 - val_loss: 1.0562 - val_f1_m: 0.7314\n",
      "Epoch 486/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0899 - f1_m: 0.7725 - val_loss: 1.0559 - val_f1_m: 0.7338\n",
      "Epoch 487/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0928 - f1_m: 0.7807 - val_loss: 1.0557 - val_f1_m: 0.7305\n",
      "Epoch 488/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0925 - f1_m: 0.7835 - val_loss: 1.0554 - val_f1_m: 0.7305\n",
      "Epoch 489/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0916 - f1_m: 0.7786 - val_loss: 1.0551 - val_f1_m: 0.7305\n",
      "Epoch 490/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0948 - f1_m: 0.7748 - val_loss: 1.0547 - val_f1_m: 0.7335\n",
      "Epoch 491/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0935 - f1_m: 0.7693 - val_loss: 1.0543 - val_f1_m: 0.7333\n",
      "Epoch 492/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0870 - f1_m: 0.7788 - val_loss: 1.0540 - val_f1_m: 0.7321\n",
      "Epoch 493/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0946 - f1_m: 0.7631 - val_loss: 1.0537 - val_f1_m: 0.7302\n",
      "Epoch 494/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0952 - f1_m: 0.7663 - val_loss: 1.0536 - val_f1_m: 0.7405\n",
      "Epoch 495/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0910 - f1_m: 0.7775 - val_loss: 1.0533 - val_f1_m: 0.7405\n",
      "Epoch 496/500\n",
      "224/224 [==============================] - 56s 251ms/step - loss: 1.0993 - f1_m: 0.7704 - val_loss: 1.0529 - val_f1_m: 0.7444\n",
      "Epoch 497/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0948 - f1_m: 0.7771 - val_loss: 1.0526 - val_f1_m: 0.7426\n",
      "Epoch 498/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0973 - f1_m: 0.7570 - val_loss: 1.0525 - val_f1_m: 0.7302\n",
      "Epoch 499/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0929 - f1_m: 0.7713 - val_loss: 1.0520 - val_f1_m: 0.7321\n",
      "Epoch 500/500\n",
      "224/224 [==============================] - 56s 252ms/step - loss: 1.0905 - f1_m: 0.7693 - val_loss: 1.0518 - val_f1_m: 0.7349\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,  # 검증 데이터셋 추가\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1, \n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     class_weight=class_weights,  # class_weight 사용\n",
    "    callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87bd746d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4849/270714275.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mloss_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'upper left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0macc_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lower left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc_ax' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label = 'train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label = 'val loss')\n",
    "\n",
    "# acc_ax.plot(history.history['accuracy'], 'b', label = 'train accuracy')\n",
    "# acc_ax.plot(history.history['val_accuracy'], 'g', label = 'val accuracy')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "\n",
    "loss_ax.legend(loc = 'upper left')\n",
    "acc_ax.legend(loc = 'lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e37dd6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 13s 205ms/step - loss: 1.0325 - f1_m: 0.7353\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가\n",
    "evaluation = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b73c7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep_config,\n",
    "#                        entity = <<YOUR CODE>>,\n",
    "#                        project = <<YOUR CODE>>)\n",
    "\n",
    "# # run the sweep\n",
    "# wandb.agent(sweep_id,\n",
    "#             function=train,\n",
    "#             count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c290a5c8",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a4e86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_test_list = []\n",
    "for conv in preprocessed_test['text']:\n",
    "    conversations_test_list.append(conv)\n",
    "\n",
    "token_test = tokenizer(\n",
    "    conversations_test_list, \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=191, \n",
    ") \n",
    "\n",
    "pred = model.predict(token_test[\"input_ids\"])\n",
    "\n",
    "pred_prob = tf.nn.softmax(pred.logits, axis=-1).numpy()\n",
    "pred_class = np.argmax(pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27d8efcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 4, 4, 2, 4, 2, 4, 4, 1, 0, 3, 2, 4, 2, 2, 3, 2, 4, 3, 4, 4,\n",
       "       4, 4, 1, 4, 4, 4, 2, 4, 4, 2, 4, 1, 2, 4, 4, 4, 4, 4, 3, 4, 2, 1,\n",
       "       4, 2, 4, 4, 4, 3, 4, 2, 2, 4, 2, 4, 4, 3, 4, 2, 4, 1, 4, 4, 4, 2,\n",
       "       4, 2, 4, 4, 2, 0, 4, 4, 4, 4, 4, 2, 4, 4, 4, 3, 4, 2, 4, 0, 4, 4,\n",
       "       4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 2, 4, 4, 2, 4,\n",
       "       1, 4, 4, 2, 4, 4, 2, 4, 2, 1, 4, 4, 4, 4, 2, 4, 2, 4, 1, 4, 2, 4,\n",
       "       4, 4, 1, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 0, 4, 0, 0, 4, 2, 2, 4, 2,\n",
       "       4, 4, 4, 2, 2, 4, 4, 2, 4, 4, 3, 4, 4, 3, 4, 2, 3, 2, 2, 2, 2, 4,\n",
       "       2, 2, 2, 1, 2, 2, 2, 4, 2, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       1, 2, 4, 4, 0, 4, 4, 4, 4, 0, 4, 0, 2, 4, 4, 2, 0, 4, 4, 1, 4, 4,\n",
       "       1, 4, 4, 4, 2, 1, 2, 4, 4, 1, 2, 4, 2, 2, 2, 1, 3, 4, 4, 4, 2, 4,\n",
       "       4, 2, 2, 4, 4, 4, 2, 0, 4, 2, 4, 4, 4, 3, 4, 2, 4, 4, 2, 2, 2, 2,\n",
       "       2, 4, 4, 4, 4, 2, 0, 4, 4, 2, 2, 2, 2, 2, 2, 4, 1, 2, 4, 4, 4, 4,\n",
       "       4, 2, 4, 4, 3, 2, 2, 4, 2, 2, 4, 4, 4, 0, 4, 4, 4, 2, 4, 4, 2, 4,\n",
       "       4, 3, 1, 4, 3, 2, 4, 0, 1, 4, 4, 4, 4, 4, 0, 4, 4, 2, 2, 2, 4, 0,\n",
       "       4, 4, 4, 4, 1, 2, 2, 4, 4, 4, 0, 2, 4, 3, 4, 4, 4, 4, 0, 2, 4, 3,\n",
       "       1, 4, 1, 4, 4, 2, 4, 4, 2, 2, 4, 4, 2, 2, 4, 4, 2, 3, 4, 2, 0, 2,\n",
       "       2, 1, 4, 2, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 2,\n",
       "       4, 2, 1, 4, 2, 2, 2, 2, 2, 4, 2, 4, 4, 0, 2, 3, 3, 4, 4, 4, 3, 4,\n",
       "       2, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 0, 4, 2, 1, 2, 4, 4, 4, 4, 4, 4,\n",
       "       4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 2, 4, 1, 2, 4, 2,\n",
       "       2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 3, 2, 2, 4, 4, 4, 4, 4, 4, 1, 2,\n",
       "       1, 2, 4, 4, 4, 2, 4, 4, 0, 1, 2, 4, 2, 2, 2, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "060f9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(y_pred, user_name, f1_score=None):\n",
    "    data_path =\"../data\"\n",
    "    save_path =\"../submission\"\n",
    "    submission_path = join(data_path, 'new_submission.csv')\n",
    "    submission = pd.read_csv(submission_path)\n",
    "    submission['class'] = y_pred\n",
    "    submission_csv_path = '{}/submission_{}_f1score_{}.csv'.format(save_path, user_name, f1_score)\n",
    "    submission.to_csv(submission_csv_path, index=False)\n",
    "    print('{} saved!'.format(submission_csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2848736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../submission/submission_YJH_tfbert_full_f1score_0.73.csv saved!\n"
     ]
    }
   ],
   "source": [
    "save_submission(pred_class,'YJH_tfbert_full',0.73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb8ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
